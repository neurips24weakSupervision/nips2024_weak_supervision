{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59554225",
   "metadata": {},
   "source": [
    "# Beta-VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e3507af",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linear_model\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_beta_vae_sklearn\u001b[39m(ground_truth_labels,\n\u001b[1;32m     30\u001b[0m                              representations,\n\u001b[1;32m     31\u001b[0m                              random_state,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m                              num_train\u001b[38;5;241m=\u001b[39mgin\u001b[38;5;241m.\u001b[39mREQUIRED,\n\u001b[1;32m     36\u001b[0m                              num_eval\u001b[38;5;241m=\u001b[39mgin\u001b[38;5;241m.\u001b[39mREQUIRED):\n\u001b[1;32m     37\u001b[0m   \u001b[38;5;124;03m\"\"\"Computes the BetaVAE disentanglement metric using scikit-learn.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m      eval_accuracy: Accuracy on evaluation set.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gin'"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2018 The DisentanglementLib Authors.  All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Implementation of the disentanglement metric from the BetaVAE paper.\n",
    "\n",
    "Based on \"beta-VAE: Learning Basic Visual Concepts with a Constrained\n",
    "Variational Framework\" (https://openreview.net/forum?id=Sy2fzU9gl).\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from absl import logging\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "import gin.tf\n",
    "\n",
    "def compute_beta_vae_sklearn(ground_truth_labels,\n",
    "                             representations,\n",
    "                             random_state,\n",
    "                             num_variables,\n",
    "                             artifact_dir=None,\n",
    "                             batch_size=gin.REQUIRED,\n",
    "                             num_train=gin.REQUIRED,\n",
    "                             num_eval=gin.REQUIRED):\n",
    "  \"\"\"Computes the BetaVAE disentanglement metric using scikit-learn.\n",
    "\n",
    "  Args:\n",
    "    ground_truth_data: GroundTruthData to be sampled from.\n",
    "    representation_function: Function that takes observations as input and\n",
    "      outputs a dim_representation sized representation for each observation.\n",
    "    random_state: Numpy random state used for randomness.\n",
    "    artifact_dir: Optional path to directory where artifacts can be saved.\n",
    "    batch_size: Number of points to be used to compute the training_sample.\n",
    "    num_train: Number of points used for training.\n",
    "    num_eval: Number of points used for evaluation.\n",
    "\n",
    "  Returns:\n",
    "    Dictionary with scores:\n",
    "      train_accuracy: Accuracy on training set.\n",
    "      eval_accuracy: Accuracy on evaluation set.\n",
    "  \"\"\"\n",
    "  del artifact_dir\n",
    "  train_points, train_labels = _generate_training_batch(\n",
    "      ground_truth_labels, representations, batch_size, num_train,\n",
    "      random_state, num_variables)\n",
    "\n",
    "  model = linear_model.LogisticRegression(random_state=random_state)\n",
    "  model.fit(train_points, train_labels)\n",
    "\n",
    "  logging.info(\"Evaluate training set accuracy.\")\n",
    "  train_accuracy = model.score(train_points, train_labels)\n",
    "  train_accuracy = np.mean(model.predict(train_points) == train_labels)\n",
    "\n",
    "  eval_points, eval_labels = _generate_training_batch(\n",
    "      ground_truth_labels, representations, batch_size, num_eval,\n",
    "      random_state,num_variables)\n",
    "\n",
    "  logging.info(\"Evaluate evaluation set accuracy.\")\n",
    "  eval_accuracy = model.score(eval_points, eval_labels)\n",
    "  logging.info(\"Evaluation set accuracy: %.2g\", eval_accuracy)\n",
    "  scores_dict = {}\n",
    "  scores_dict[\"train_accuracy\"] = train_accuracy\n",
    "  scores_dict[\"eval_accuracy\"] = eval_accuracy\n",
    "  return scores_dict\n",
    "\n",
    "\n",
    "def _generate_training_batch(ground_truth_labels, representations,\n",
    "                             batch_size, num_points, random_state,num_variables):\n",
    "  \"\"\"Sample a set of training samples based on a batch of ground-truth data.\n",
    "\n",
    "  Args:\n",
    "    ground_truth_data: GroundTruthData to be sampled from.\n",
    "    representation_function: Function that takes observations as input and\n",
    "      outputs a dim_representation sized representation for each observation.\n",
    "    batch_size: Number of points to be used to compute the training_sample.\n",
    "    num_points: Number of points to be sampled for training set.\n",
    "    random_state: Numpy random state used for randomness.\n",
    "\n",
    "  Returns:\n",
    "    points: (num_points, dim_representation)-sized numpy array with training set\n",
    "      features.\n",
    "    labels: (num_points)-sized numpy array with training set labels.\n",
    "  \"\"\"\n",
    "  points = None  # Dimensionality depends on the representation function.\n",
    "  labels = np.zeros(num_points, dtype=np.int64)\n",
    "  for i in range(num_points):\n",
    "    labels[i], feature_vector = _generate_training_sample(\n",
    "        ground_truth_labels, representations, batch_size, random_state,num_variables)\n",
    "    if points is None:\n",
    "      points = np.zeros((num_points, feature_vector.shape[0]))\n",
    "    points[i, :] = feature_vector\n",
    "  return points, labels\n",
    "\n",
    "#start and end points for each property\n",
    "def _sample_factors_min_max(index):\n",
    "  #size fixed\n",
    "  if index == 0:\n",
    "    min = 3\n",
    "    max = 6\n",
    "  #texture fixed\n",
    "  if index == 1:\n",
    "    min = 6\n",
    "    max = 66\n",
    "  #shape fixed\n",
    "  if index == 2:\n",
    "    min = 66\n",
    "    max = 70\n",
    "  return min,max\n",
    "\n",
    "#get a representation that fits the fixed label\n",
    "def _getRandomRepresentation(labels, datapoints, label, randomState):\n",
    "  start = randomState.randint(len(datapoints))\n",
    "  start_pos = start\n",
    "  while start < len(labels) and labels[start][label] != 1:\n",
    "    start = start + 1\n",
    "    if start == len(labels):\n",
    "      start = 0\n",
    "  return datapoints[start], start\n",
    "\n",
    "#sample fixed factors\n",
    "def _sample_factors(ground_truth_labels, representations, batch_size, random_state,index):\n",
    "  min ,max = _sample_factors_min_max(index)\n",
    "  representations1 = []\n",
    "  representations2 = []\n",
    "  for i in range(batch_size):\n",
    "    randomLabel = random_state.randint(min,max)\n",
    "    representation1, returnIndex = _getRandomRepresentation(ground_truth_labels, representations,randomLabel, random_state)\n",
    "    representation2, returnIndex = _getRandomRepresentation(ground_truth_labels, representations,randomLabel, random_state)\n",
    "    representations1.append(representation1)\n",
    "    representations2.append(representation2)\n",
    "  return np.array(representations1), np.array(representations2)\n",
    "\n",
    "\n",
    "def _generate_training_sample(ground_truth_labels, representations,\n",
    "                              batch_size, random_state, num_variables):\n",
    "  \"\"\"Sample a single training sample based on a mini-batch of ground-truth data.\n",
    "\n",
    "  Args:\n",
    "    ground_truth_data: GroundTruthData to be sampled from.\n",
    "    representation_function: Function that takes observation as input and\n",
    "      outputs a representation.\n",
    "    batch_size: Number of points to be used to compute the training_sample\n",
    "    random_state: Numpy random state used for randomness.\n",
    "\n",
    "  Returns:\n",
    "    index: Index of coordinate to be used.\n",
    "    feature_vector: Feature vector of training sample.\n",
    "  \"\"\"\n",
    "  # Select random coordinate to keep fixed.\n",
    "  index = random_state.randint(num_variables)\n",
    "  # Sample two mini batches of latent variables.\n",
    "  representation1, representation2 = _sample_factors(ground_truth_labels, representations,batch_size, random_state, index)\n",
    "  # Ensure sampled coordinate is the same across pairs of samples.\n",
    "\n",
    "  # Compute the feature vector based on differences in representation.\n",
    "  feature_vector = np.mean(np.abs(representation1 - representation2), axis=0)\n",
    "  return index, feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015353c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execute with:\n",
    "random_state = np.random.RandomState(10)\n",
    "num_variables = 3\n",
    "path_name \"YOUR_PATH\"\n",
    "labels = np.load(path_name + \"/binded_properties.npy\")\n",
    "slots =  np.load(path_name + \"/binded_slots.npy\")\n",
    "batch_size = 10\n",
    "num_train = 1000\n",
    "num_test = 1000\n",
    "\n",
    "compute_beta_vae_sklearn(labels,slots,random_state,num_variables,artifact_dir=None,batch_size=batch_size,num_train=num_train,num_eval=num_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e905f8b6",
   "metadata": {},
   "source": [
    "# DCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7952d513",
   "metadata": {},
   "outputs": [],
   "source": [
    "#argmax over each property\n",
    "def transformLabel(label):\n",
    "  newLabel = []\n",
    "  newLabel.append(np.argmax(label[3:6])) \n",
    "  newLabel.append(np.argmax(label[6:71-5]))\n",
    "  newLabel.append(np.argmax(label[71-5:71-1]))\n",
    "  return np.array(newLabel)\n",
    "\n",
    "\n",
    "def _generate_batch_factor_code(slots, labels,\n",
    "                               num_points,random_state, batch_size):\n",
    "  \"\"\"Sample a single training sample based on a mini-batch of ground-truth data.\n",
    "\n",
    "  Args:\n",
    "    ground_truth_data: GroundTruthData to be sampled from.\n",
    "    representation_function: Function that takes observation as input and\n",
    "      outputs a representation.\n",
    "    num_points: Number of points to sample.\n",
    "    random_state: Numpy random state used for randomness.\n",
    "    batch_size: Batchsize to sample points.\n",
    "\n",
    "  Returns:\n",
    "    representations: Codes (num_codes, num_points)-np array.\n",
    "    factors: Factors generating the codes (num_factors, num_points)-np array.\n",
    "  \"\"\"\n",
    "  representations = None\n",
    "  factors = None\n",
    "  i = 0\n",
    "  while i < num_points:\n",
    "    randomPoint = random_state.randint(len(slots))\n",
    "    slot_batched = slots[randomPoint]\n",
    "    label_batched = labels[randomPoint]\n",
    "    label_batched = transformLabel(label_batched)\n",
    "    current_factors = np.array([label_batched])\n",
    "    current_slot = np.array([slot_batched])\n",
    "    if i == 0:\n",
    "      factors = current_factors\n",
    "      representations = current_slot\n",
    "    else:\n",
    "      factors = np.vstack((factors, current_factors))\n",
    "      representations = np.vstack((representations,current_slot))\n",
    "    i = i + 1\n",
    "  return np.transpose(representations), np.transpose(factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b647ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2018 The DisentanglementLib Authors.  All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Implementation of Disentanglement, Completeness and Informativeness.\n",
    "\n",
    "Based on \"A Framework for the Quantitative Evaluation of Disentangled\n",
    "Representations\" (https://openreview.net/forum?id=By-7dz-AZ).\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from absl import logging\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn import ensemble\n",
    "import gin.tf\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "def compute_dci(slots, labels, random_state,\n",
    "                num_train,\n",
    "                num_test,\n",
    "                batch_size=16,\n",
    "                artifact_dir=None):\n",
    "  \"\"\"Computes the DCI scores according to Sec 2.\n",
    "\n",
    "  Args:\n",
    "    ground_truth_data: GroundTruthData to be sampled from.\n",
    "    representation_function: Function that takes observations as input and\n",
    "      outputs a dim_representation sized representation for each observation.\n",
    "    random_state: Numpy random state used for randomness.\n",
    "    artifact_dir: Optional path to directory where artifacts can be saved.\n",
    "    num_train: Number of points used for training.\n",
    "    num_test: Number of points used for testing.\n",
    "    batch_size: Batch size for sampling.\n",
    "\n",
    "  Returns:\n",
    "    Dictionary with average disentanglement score, completeness and\n",
    "      informativeness (train and test).\n",
    "  \"\"\"\n",
    "  del artifact_dir\n",
    "  logging.info(\"Generating training set.\")\n",
    "  # mus_train are of shape [num_codes, num_train], while ys_train are of shape\n",
    "  # [num_factors, num_train].\n",
    "  mus_train, ys_train = _generate_batch_factor_code(\n",
    "      slots, labels, num_train,\n",
    "      random_state, batch_size)\n",
    "  assert mus_train.shape[1] == num_train\n",
    "  assert ys_train.shape[1] == num_train\n",
    "  mus_test, ys_test = _generate_batch_factor_code(\n",
    "      slots, labels, num_test,\n",
    "      random_state, batch_size)\n",
    "  print(ys_test.shape)\n",
    "  scores = _compute_dci(mus_train, ys_train, mus_test, ys_test)\n",
    "  return scores\n",
    "\n",
    "\n",
    "def _compute_dci(mus_train, ys_train, mus_test, ys_test):\n",
    "  \"\"\"Computes score based on both training and testing codes and factors.\"\"\"\n",
    "  scores = {}\n",
    "  importance_matrix, train_err, test_err = compute_importance_gbt(\n",
    "      mus_train, ys_train, mus_test, ys_test)\n",
    "  assert importance_matrix.shape[0] == mus_train.shape[0]\n",
    "  assert importance_matrix.shape[1] == ys_train.shape[0]\n",
    "  scores[\"informativeness_train\"] = train_err\n",
    "  scores[\"informativeness_test\"] = test_err\n",
    "  scores[\"disentanglement\"] = disentanglement(importance_matrix)\n",
    "  scores[\"completeness\"] = completeness(importance_matrix)\n",
    "  return scores\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "\n",
    "\n",
    "def compute_importance_gbt(x_train, y_train, x_test, y_test):\n",
    "  \"\"\"Compute importance based on gradient boosted trees.\"\"\"\n",
    "  num_factors = y_train.shape[0]\n",
    "  num_codes = x_train.shape[0]\n",
    "  importance_matrix = np.zeros(shape=[num_codes, num_factors],\n",
    "                               dtype=np.float64)\n",
    "  train_loss = []\n",
    "  test_loss = []\n",
    "  for i in range(num_factors):\n",
    "    print(i)\n",
    "    model = ensemble.GradientBoostingClassifier()\n",
    "    #model = DecisionTreeClassifier()\n",
    "    #model = RandomForestClassifier()\n",
    "    model.fit(x_train.T, y_train[i, :])\n",
    "    importance_matrix[:, i] = np.abs(model.feature_importances_)\n",
    "    train_loss.append(np.mean(model.predict(x_train.T) == y_train[i, :]))\n",
    "    test_loss.append(np.mean(model.predict(x_test.T) == y_test[i, :]))\n",
    "  return importance_matrix, np.mean(train_loss), np.mean(test_loss)\n",
    "\n",
    "\n",
    "def disentanglement_per_code(importance_matrix):\n",
    "  \"\"\"Compute disentanglement score of each code.\"\"\"\n",
    "  # importance_matrix is of shape [num_codes, num_factors].\n",
    "  return 1. - scipy.stats.entropy(importance_matrix.T + 1e-11,\n",
    "                                  base=importance_matrix.shape[1])\n",
    "\n",
    "\n",
    "def disentanglement(importance_matrix):\n",
    "  \"\"\"Compute the disentanglement score of the representation.\"\"\"\n",
    "  per_code = disentanglement_per_code(importance_matrix)\n",
    "  if importance_matrix.sum() == 0.:\n",
    "    importance_matrix = np.ones_like(importance_matrix)\n",
    "  code_importance = importance_matrix.sum(axis=1) / importance_matrix.sum()\n",
    "\n",
    "  return np.sum(per_code*code_importance)\n",
    "\n",
    "\n",
    "def completeness_per_factor(importance_matrix):\n",
    "  \"\"\"Compute completeness of each factor.\"\"\"\n",
    "  # importance_matrix is of shape [num_codes, num_factors].\n",
    "  return 1. - scipy.stats.entropy(importance_matrix + 1e-11,\n",
    "                                  base=importance_matrix.shape[0])\n",
    "\n",
    "\n",
    "def completeness(importance_matrix):\n",
    "  \"\"\"\"Compute completeness of the representation.\"\"\"\n",
    "  per_factor = completeness_per_factor(importance_matrix)\n",
    "  if importance_matrix.sum() == 0.:\n",
    "    importance_matrix = np.ones_like(importance_matrix)\n",
    "  factor_importance = importance_matrix.sum(axis=0) / importance_matrix.sum()\n",
    "  return np.sum(per_factor*factor_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761952a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execute with:\n",
    "random_state = np.random.RandomState(10)\n",
    "num_variables = 3\n",
    "path_name \"YOUR_PATH\"\n",
    "labels = np.load(path_name + \"/binded_properties.npy\")\n",
    "slots =  np.load(path_name + \"/binded_slots.npy\")\n",
    "batch_size = 10\n",
    "num_train = 1000\n",
    "num_test = 1000\n",
    "\n",
    "compute_dci(slots, labels, rando_state,num_train,num_test,batch_size=batch_size,artifact_dir=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
